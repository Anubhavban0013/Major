{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Under Apache License Version 2.0\n",
    "# @Hardik Vasa\n",
    "#\n",
    "\n",
    "#Import Libraries\n",
    "import time     #For Delay calculations\n",
    "import sys    #for system related information\n",
    "from subprocess import Popen, PIPE\n",
    "import subprocess\n",
    "import re\n",
    "import json as m_json\n",
    "import socket\n",
    "import urllib\n",
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "    from urlparse import urlparse\n",
    "try:\n",
    "    import urllib.request       #Python 3.x\n",
    "except ImportError:\n",
    "    import urllib2      #Python 2.x\n",
    "###### End of Import ######\n",
    "\n",
    "\n",
    "\n",
    "###### Get IP of a website from the URL ######\n",
    "def get_ip(url):\n",
    "    ip = socket.gethostbyname(url)\n",
    "    return ip\n",
    "\n",
    "\n",
    "\n",
    "###### Ping we Website (ICMP Ping) ######\n",
    "def ping(host):\n",
    "    ping = subprocess.Popen(\n",
    "        [\"ping\", \"-v\", \"4\", host],\n",
    "        stdout = subprocess.PIPE,\n",
    "        stderr = subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    out, error = ping.communicate()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "###### Traceroute to a website ######\n",
    "def traceroute(url,*arg):\n",
    "    while True:\n",
    "        if 'http' not in url:\n",
    "            url = \"http://\" + url\n",
    "        elif \"www\" not in url:\n",
    "            url = \"www.\"[:7] + url[7:]\n",
    "        else:\n",
    "            url = url\n",
    "            break\n",
    "    url = urlparse(url)\n",
    "    url = url.netloc\n",
    "    print(url)\n",
    "    p = Popen(['tracert', url], stdout=PIPE)\n",
    "    while True:\n",
    "        line = p.stdout.readline()\n",
    "        line2 = str(line).replace('\\\\r','').replace('\\\\n','')\n",
    "        if len(arg)>0:\n",
    "            file = open(arg[0], \"a\")\n",
    "            file.write(line2)\n",
    "            file.close()\n",
    "        print(line2)\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "###### WHOIS Lookup ######\n",
    "#Perform a generic whois query to a server and get the reply\n",
    "def perform_whois(server , query) :\n",
    "    #socket connection\n",
    "    s = socket.socket(socket.AF_INET , socket.SOCK_STREAM)\n",
    "    s.connect((server , 43))\n",
    "\n",
    "    s.send(query + '\\r\\n')  #send data\n",
    "\n",
    "    message = ''        #receive reply\n",
    "    while len(message) < 10000:\n",
    "        raw = s.recv(100)\n",
    "        if(raw == ''):\n",
    "            break\n",
    "        message = message + raw\n",
    "\n",
    "    return message\n",
    "\n",
    "#Function to perform the whois on a domain name\n",
    "def get_whois_data(domain):\n",
    "    #remove scheme(http) and 'www'\n",
    "    domain = domain.replace('http://','')\n",
    "    domain = domain.replace('www.','')\n",
    "\n",
    "    #get the extension , .com , .org , .edu\n",
    "    ext = domain[-3:]\n",
    "\n",
    "    #If top level domain .com .org .net\n",
    "    if(ext == 'com' or ext == 'org' or ext == 'net'):\n",
    "        whois = 'whois.internic.net'\n",
    "        msg = perform_whois(whois , domain)\n",
    "\n",
    "        #Now scan the reply for the whois server\n",
    "        lines = msg.splitlines()\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                words = line.split(':')\n",
    "                if  'Whois' in words[0] and 'whois.' in words[1]:\n",
    "                    whois = words[1].strip()\n",
    "                    break;\n",
    "\n",
    "    #Or Regional/Country level - contact whois.iana.org to find the whois server of a particular TLD\n",
    "    else:\n",
    "        #Break again like , co.in to in\n",
    "        ext = domain.split('.')[-1]\n",
    "\n",
    "        whois = 'whois.iana.org'  #Give the Whois server for the particular country\n",
    "        msg = perform_whois(whois , ext)\n",
    "\n",
    "        lines = msg.splitlines()   #Get the reply for a whois server\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                words = line.split(':')\n",
    "                if 'whois.' in words[1] and 'Whois Server (port 43)' in words[0]:\n",
    "                    whois = words[1].strip()\n",
    "                    break;\n",
    "\n",
    "    msg = perform_whois(whois , domain) #Get reply from the final whois server\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "\n",
    "###### Download HTML Page Main Function ######\n",
    "#Downloading entire Web Document (Raw Page Content) for the crawler\n",
    "def download_page(url,*arg):\n",
    "    version = (3,0)\n",
    "    cur_version = sys.version_info\n",
    "    if cur_version >= version:     #If the Current Version of Python is 3.0 or above\n",
    "        try:\n",
    "            headers = {}\n",
    "            headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n",
    "            req = urllib.request.Request(url, headers = headers)\n",
    "            resp = urllib.request.urlopen(req)\n",
    "            page = str(resp.read())\n",
    "            if len(arg)>0:\n",
    "                file = open(arg[0], \"w\")\n",
    "                file.write(page)\n",
    "                file.close()\n",
    "                return page\n",
    "            else:\n",
    "                return page\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    else:                        #If the Current Version of Python is 2.x\n",
    "        try:\n",
    "            headers = {}\n",
    "            headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n",
    "            req = urllib2.Request(url, headers = headers)\n",
    "            response = urllib2.urlopen(req)\n",
    "            page = response.read()\n",
    "            if len(arg)>0:\n",
    "                file = open(arg[0], \"w\")\n",
    "                file.write(page)\n",
    "                file.close()\n",
    "                return page\n",
    "            else:\n",
    "                return page\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "\n",
    "\n",
    "###### Extract the title tag ######\n",
    "def page_title(url):\n",
    "    page = download_page(url)\n",
    "    start_title = page.find(\"<title\")\n",
    "    end_start_title = page.find(\">\",start_title+1)\n",
    "    stop_title = page.find(\"</title>\", end_start_title + 1)\n",
    "    title = page[end_start_title + 1 : stop_title]\n",
    "    return (title)\n",
    "\n",
    "\n",
    "\n",
    "###### Check for URL extension so crawler discards non-html pages ######\n",
    "def extension_scan(url):\n",
    "    a = ['.png','.jpg','.jpeg','.gif','.tif','.txt','.svg','.pdf']\n",
    "    j = 0\n",
    "    while j < (len(a)):\n",
    "        if a[j] in url:\n",
    "            flag2 = 1\n",
    "            break\n",
    "        else:\n",
    "            flag2 = 0\n",
    "            j = j+1\n",
    "    #print(flag2)\n",
    "    return flag2\n",
    "\n",
    "\n",
    "\n",
    "###### URL Normalizer for the Users ######\n",
    "#URL parsing for incomplete or duplicate URLs for users\n",
    "def url_normalize(url,seed_page):\n",
    "    url = url.lower()    #Make it lower case\n",
    "    s = urlparse(url)       #parse the given url\n",
    "    seed_page = seed_page.lower()       #Make it lower case\n",
    "    t = urlparse(seed_page)     #parse the seed page (reference page)\n",
    "    flag = 0\n",
    "    if url == \"/\":\n",
    "        url = seed_page\n",
    "        flag = 0\n",
    "        print (url)\n",
    "    if s.netloc == \"\":\n",
    "        path = url.find('/')\n",
    "        if path != -1:\n",
    "            url = url[path:]\n",
    "            url = seed_page + url\n",
    "            flag = 0\n",
    "            s = urlparse(url)\n",
    "        else:\n",
    "            url = url\n",
    "            flag = 0\n",
    "    if not s.scheme:\n",
    "        url = \"http://\" + url\n",
    "        flag = 0\n",
    "        s = urlparse(url)\n",
    "    if \"#\" in url:\n",
    "        url = url[:url.find(\"#\")]\n",
    "    if \"?\" in url:\n",
    "        url = url[:url.find(\"?\")]\n",
    "    if \"www\" not in url:\n",
    "        url = \"www.\"[:7] + url[7:]\n",
    "        flag = 0\n",
    "        s = urlparse(url)\n",
    "    if 'http' not in url:\n",
    "        url = \"http://\" + url\n",
    "        flag = 0\n",
    "        s = urlparse(url)\n",
    "    if url[len(url)-1] == \"/\":\n",
    "        url = url[:-1]\n",
    "        flag = 0\n",
    "    if s.netloc != t.netloc:\n",
    "        s = urlparse(url)\n",
    "        url = url\n",
    "        flag = 1\n",
    "    if flag == 0:\n",
    "        return url\n",
    "    else:\n",
    "        return \"Invalid URL\"\n",
    "\n",
    "\n",
    "\n",
    "###### Find all the links function for users ######\n",
    "#Finding 'Next Link' on a given web page for users\n",
    "def find_next_link(s):\n",
    "    start_link = s.find(\"<a href\")\n",
    "    if start_link == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_quote = s.find('\"', start_link)\n",
    "        end_quote = s.find('\"',start_quote+1)\n",
    "        link = str(s[start_quote+1:end_quote])\n",
    "        return link, end_quote\n",
    "\n",
    "#Getting all links as list with the help of 'get_next_links' for users\n",
    "def find_all_links(content):\n",
    "    if content.startswith('http') or content.startswith('www'):\n",
    "        url = content\n",
    "        if \"http\" not in url:\n",
    "            url = \"http://\" + url\n",
    "        if \"www\" not in url:\n",
    "            url = \"www.\"[:7] + url[7:]\n",
    "        content = download_page(url)\n",
    "    page = content\n",
    "    links = []\n",
    "    while True:\n",
    "        link, end_link = find_next_link(page)\n",
    "        if link == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            links.append(link)      #Append all the links in the list named 'Links'\n",
    "            #time.sleep(0.1)\n",
    "            page = page[end_link:]\n",
    "    return links\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#URL parsing for incomplete or duplicate URLs for crawler\n",
    "def url_parse(url,seed_page):\n",
    "    url = url.lower().replace(' ','%20')    #Make it lower case\n",
    "    s = urlparse(url)       #parse the given url\n",
    "    t = urlparse(seed_page)     #parse the seed page (reference page)\n",
    "    i = 0\n",
    "    while i<=7:\n",
    "        if url == \"/\":\n",
    "            url = seed_page\n",
    "            flag = 0\n",
    "        elif not s.scheme:\n",
    "            url = \"http://\" + url\n",
    "            flag = 0\n",
    "        elif \"#\" in url:\n",
    "            url = url[:url.find(\"#\")]\n",
    "        elif \"?\" in url:\n",
    "            url = url[:url.find(\"?\")]\n",
    "        elif s.netloc == \"\":\n",
    "            url = seed_page + s.path\n",
    "            flag = 0\n",
    "        elif \"www\" not in url:\n",
    "            url = \"www.\"[:7] + url[7:]\n",
    "            flag = 0\n",
    "\n",
    "        elif url[len(url)-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "            flag = 0\n",
    "        elif s.netloc != t.netloc:\n",
    "            url = url\n",
    "            flag = 1\n",
    "            break\n",
    "        else:\n",
    "            url = url\n",
    "            flag = 0\n",
    "            break\n",
    "\n",
    "        i = i+1\n",
    "        s = urlparse(url)   #Parse after every loop to update the values of url parameters\n",
    "    return(url, flag)\n",
    "\n",
    "\n",
    "\n",
    "#Main function that crawls the entire web (out of domain) in breath first order\n",
    "def web_crawl(*arg):\n",
    "\n",
    "    to_crawl = [arg[0]]      #Define list name 'Seed Page'\n",
    "    crawled=[]      #Define list name 'Seed Page'\n",
    "\n",
    "    a = urlparse(arg[0])\n",
    "    seed_page = a.scheme+\"://\"+a.netloc\n",
    "\n",
    "    i=0;        #Initiate Variable to count No. of Iterations\n",
    "    while to_crawl:     #Continue Looping till the 'to_crawl' list is not empty\n",
    "        urll = to_crawl.pop(0)      #If there are elements in to_crawl then pop out the first element\n",
    "\n",
    "        urll,flag = url_parse(urll,seed_page)\n",
    "        flag2 = extension_scan(urll)\n",
    "\n",
    "        #If flag = 1, then the URL is outside the seed domain URL\n",
    "        if flag2 == 1:\n",
    "            pass        #Do Nothing\n",
    "\n",
    "        else:\n",
    "            if urll in crawled:     #Else check if the URL is already crawled\n",
    "                pass        #Do Nothing\n",
    "            else:       #If the URL is not already crawled, then crawl i and extract all the links from it\n",
    "                print(\"\\n\"+urll)\n",
    "                if len(arg)>1:\n",
    "                    delay = arg[1]\n",
    "                    time.sleep(delay)\n",
    "                #print(download_page(urll))\n",
    "                to_crawl = to_crawl + find_all_links(download_page(urll))\n",
    "                crawled.append(urll)\n",
    "\n",
    "                #Remove duplicated from to_crawl\n",
    "                n = 1\n",
    "                j = 0\n",
    "                #k = 0\n",
    "                while j < (len(to_crawl)-n):\n",
    "                    if to_crawl[j] in to_crawl[j+1:(len(to_crawl)-1)]:\n",
    "                        to_crawl.pop(j)\n",
    "                        n = n+1\n",
    "                    else:\n",
    "                        pass     #Do Nothing\n",
    "                    j = j+1\n",
    "            i=i+1    #Iteration Counter\n",
    "\n",
    "            #print(to_crawl)\n",
    "            print(\"Iteration No. = \" + str(i))\n",
    "            print(\"Pages to Crawl = \" + str(len(to_crawl)))\n",
    "            print(\"Pages Crawled = \" + str(len(crawled)))\n",
    "\n",
    "            if len(arg)>1:\n",
    "                if arg[2]==\"write_log\":\n",
    "                    file = open('log.txt', 'a')        #Open the text file called database.txt\n",
    "                    file.write(\"URL: \" + urll + \"\\n\")         #Write the title of the page\n",
    "                    file.write(\"Iteration No. = \" + str(i) + \"\\n\")\n",
    "                    file.write(\"Pages to Crawl = \" + str(len(to_crawl)) + \"\\n\")\n",
    "                    file.write(\"Pages Crawled = \" + str(len(crawled)) + \"\\n\\n\")\n",
    "                    file.close()                            #Close the file\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Main function crawls the entire site (in-domain) in breath first order\n",
    "def web_crawl_in_domain(*arg):\n",
    "\n",
    "    to_crawl = [arg[0]]      #Define list name 'Seed Page'\n",
    "    crawled=[]      #Define list name 'Seed Page'\n",
    "\n",
    "    a = urlparse(arg[0])\n",
    "    seed_page = a.scheme+\"://\"+a.netloc\n",
    "\n",
    "    i=0;        #Initiate Variable to count No. of Iterations\n",
    "    while to_crawl:     #Continue Looping till the 'to_crawl' list is not empty\n",
    "        urll = to_crawl.pop(0)      #If there are elements in to_crawl then pop out the first element\n",
    "\n",
    "        urll,flag = url_parse(urll,seed_page)\n",
    "        flag2 = extension_scan(urll)\n",
    "\n",
    "        #If flag = 1, then the URL is outside the seed domain URL\n",
    "        if flag == 1 or flag2 == 1:\n",
    "            pass        #Do Nothing\n",
    "\n",
    "        else:\n",
    "            if urll in crawled:     #Else check if the URL is already crawled\n",
    "                pass        #Do Nothing\n",
    "            else:       #If the URL is not already crawled, then crawl i and extract all the links from it\n",
    "                print(\"\\n\"+urll)\n",
    "                if len(arg)>1:\n",
    "                    delay = arg[1]\n",
    "                    time.sleep(delay)\n",
    "                #print(download_page(urll))\n",
    "                to_crawl = to_crawl + find_all_links(download_page(urll))\n",
    "                crawled.append(urll)\n",
    "\n",
    "                #Remove duplicated from to_crawl\n",
    "                n = 1\n",
    "                j = 0\n",
    "                #k = 0\n",
    "                while j < (len(to_crawl)-n):\n",
    "                    if to_crawl[j] in to_crawl[j+1:(len(to_crawl)-1)]:\n",
    "                        to_crawl.pop(j)\n",
    "                        n = n+1\n",
    "                    else:\n",
    "                        pass     #Do Nothing\n",
    "                    j = j+1\n",
    "            i=i+1    #Iteration Counter\n",
    "\n",
    "            #print(to_crawl)\n",
    "            print(\"Iteration No. = \" + str(i))\n",
    "            print(\"Pages to Crawl = \" + str(len(to_crawl)))\n",
    "            print(\"Pages Crawled = \" + str(len(crawled)))\n",
    "\n",
    "            if len(arg)>1:\n",
    "                if arg[2]==\"write_log\":\n",
    "                    file = open('log.txt', 'a')        #Open the text file called database.txt\n",
    "                    file.write(\"URL: \" + urll + \"\\n\")         #Write the title of the page\n",
    "                    file.write(\"Iteration No. = \" + str(i) + \"\\n\")\n",
    "                    file.write(\"Pages to Crawl = \" + str(len(to_crawl)) + \"\\n\")\n",
    "                    file.write(\"Pages Crawled = \" + str(len(crawled)) + \"\\n\\n\")\n",
    "                    file.close()                            #Close the file\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "#Removing HTML tags from the content\n",
    "def remove_html_tags(page):\n",
    "    pure_text = (re.sub(r'<.+?>', '', page))       #From '<' to the next '>'\n",
    "    return pure_text\n",
    "\n",
    "\n",
    "\n",
    "#Clean HTML Tags\n",
    "def clean_page(page):\n",
    "    while True:\n",
    "        script_start = page.find(\"<script\")\n",
    "        script_end = page.find(\"</script>\")\n",
    "        if '<script' in page:\n",
    "            script_section = page[script_start:script_end+9]\n",
    "            page = page.replace(script_section,'')\n",
    "        else:\n",
    "            break\n",
    "    pure_text = (re.sub(r'<.+?>', '', page))#.replace('\\n', '')\n",
    "    return pure_text\n",
    "\n",
    "\n",
    "\n",
    "###### Extract Headings ######\n",
    "#Finding 'Next Heading' on a given web page for users\n",
    "def get_next_heading(s,heading_type):\n",
    "    start_link = s.find(\"<\"+heading_type)\n",
    "    if start_link == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_headings\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_quote = s.find('>', start_link+1)\n",
    "        end_quote = s.find('</'+heading_type+'>',start_quote+1)\n",
    "        link = str(s[start_quote+1:end_quote])\n",
    "        return link, end_quote\n",
    "\n",
    "#Getting all headings with the help of 'get_next_headings' for users\n",
    "def get_all_headings_as_list(url,heading_type):\n",
    "    links = []\n",
    "    page = download_page(url)\n",
    "    while True:\n",
    "        link, end_link = get_next_heading(page,heading_type)\n",
    "        link = link.replace('\\n',' ')\n",
    "        link = re.sub(r'<.+?>', '', link)\n",
    "        if link == \"no_headings\":\n",
    "            break\n",
    "        else:\n",
    "            links.append(link)      #Append all the links in the list named 'Links'\n",
    "            #time.sleep(0.1)\n",
    "            page = page[end_link:]\n",
    "    return links\n",
    "\n",
    "# Get all the headings from get_all_headings_as_list\n",
    "def get_all_headings(*arg):\n",
    "    url = arg[0]\n",
    "    lists = get_all_headings_as_list(url,arg[1])\n",
    "    if len(arg)>2:\n",
    "        if arg[2] == 'list':\n",
    "            print(lists)     #Display all headings as list\n",
    "    else:\n",
    "        for i in lists:    #Display all headings one below the other\n",
    "            print(i)\n",
    "\n",
    "\n",
    "\n",
    "###### Extract Paragraphs ######\n",
    "#Finding 'Next Paragraph' on a given web page for users\n",
    "def get_next_paragraph(s):\n",
    "    start_link = s.find(\"<p\")\n",
    "    if start_link == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_quote = s.find('>', start_link+1)\n",
    "        end_quote = s.find('</p>',start_quote+1)\n",
    "        link = str(s[start_quote+1:end_quote])\n",
    "        return link, end_quote\n",
    "\n",
    "#Getting all paragraphs with the help of 'get_next_paragraph_as_list' for users\n",
    "def get_all_paragraphs_as_list(url):\n",
    "    links = []\n",
    "    page = download_page(url)\n",
    "    while True:\n",
    "        link, end_link = get_next_paragraph(page)\n",
    "        link = link.replace('\\n',' ')\n",
    "        link = re.sub(r'<.+?>', '', link)\n",
    "        if link == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            links.append(link)      #Append all the links in the list named 'Links'\n",
    "            #time.sleep(0.1)\n",
    "            page = page[end_link:]\n",
    "    return links\n",
    "\n",
    "#Get all the paragraphs one below the other with the help of get_all_paragraphs_as_list for users\n",
    "def get_all_paragraphs(url):\n",
    "    lists = get_all_paragraphs_as_list(url)\n",
    "    for i in lists:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "\n",
    "######### Download Images From a Web Page and store it on hard drive #########\n",
    "#Finding 'Next Image Link' for get_all_images\n",
    "def get_next_images_link(s):\n",
    "    start_line = s.find(\"<img\")\n",
    "    if start_line == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_link = s.find('src=', start_line)\n",
    "        end_link = s.find('\"',start_link+5)\n",
    "        link = str(s[start_link+5:end_link])\n",
    "        return link, end_link\n",
    "\n",
    "#Getting all image links with the help of 'get_next_links' for get_all_images\n",
    "def get_all_images_links(url):\n",
    "    page = download_page(url)\n",
    "    links = []\n",
    "    while True:\n",
    "        link, end_link = get_next_images_link(page)\n",
    "        if link == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            links.append(link)      #Append all the links in the list named 'Links'\n",
    "            page = page[end_link:]\n",
    "    return links\n",
    "\n",
    "#Download all images in hard disk\n",
    "def get_all_images(*arg):\n",
    "    url = arg[0]\n",
    "    import urllib\n",
    "    links = get_all_images_links(url)\n",
    "    print(links)\n",
    "    if len(arg)>1 and arg[1] == \"download\":\n",
    "        s = urlparse(url)\n",
    "        seed_page = s.scheme+'://'+s.netloc\n",
    "        i = 0\n",
    "        while i<len(links):\n",
    "            link,flag = url_parse(links[i],seed_page)\n",
    "            print(\"downloading --> \"+link)\n",
    "            try:\n",
    "                file = urllib.URLopener()\n",
    "                file.retrieve(link, str(\"img \"+str(i)+\".jpg\"))\n",
    "            except:\n",
    "                pass\n",
    "            i = i+1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "############## Download Google Images ############\n",
    "#Finding 'Next Image' from the given raw page for users (image search)\n",
    "def get_next_image_link(s):\n",
    "    start_line = s.find('rg_di')\n",
    "    if start_line == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_images\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_line = s.find('\"class=\"rg_di\"')\n",
    "        start_content = s.find('imgurl=',start_line+1)\n",
    "        end_content = s.find('&amp;',start_content+1)\n",
    "        content_raw = str(s[start_content+7:end_content])\n",
    "        return content_raw, end_content\n",
    "\n",
    "\n",
    "#Getting all links with the help of 'get_next_image_link'\n",
    "def get_all_image_links(page):\n",
    "    items = []\n",
    "    while True:\n",
    "        item, end_content = get_next_image_link(page)\n",
    "        if item == \"no_images\":\n",
    "            break\n",
    "        else:\n",
    "            items.append(item)      #Append all the links in the list named 'Links'\n",
    "            #time.sleep(0.1)        #Timer could be used to slow down the request for image downloads\n",
    "            page = page[end_content:]\n",
    "    return items\n",
    "\n",
    "#Download Images\n",
    "def download_google_images(*arg):\n",
    "    import urllib\n",
    "    search_keyword = arg[0]\n",
    "    result = (str(type(search_keyword)))\n",
    "    if 'list' in result:\n",
    "        i= 0\n",
    "        while i<len(search_keyword):\n",
    "            items = []\n",
    "            iteration = \"Item no.: \" + str(i+1) + \" -->\" + \" Item name = \" + str(search_keyword[i])\n",
    "            print (iteration)\n",
    "            search_keywords = search_keyword[i]\n",
    "            search = search_keywords.replace(' ','%20')\n",
    "\n",
    "            url = 'https://www.google.com/search?q=' + search +  '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "            raw_html =  (download_page(url))\n",
    "            items = items + (get_all_image_links(raw_html))\n",
    "\n",
    "            print (\"Image Links = \"+str(items))\n",
    "            print (\"Total Image Links = \"+str(len(items)))\n",
    "            print (\"\\n\")\n",
    "            i = i+1\n",
    "\n",
    "            info = open('output.txt', 'a')        #Open the text file called database.txt\n",
    "            info.write(str(i) + ': ' + str(search_keyword[i-1]) + \": \" + str(items) + \"\\n\\n\\n\")     #Write the title of the page\n",
    "            info.close()                            #Close the file\n",
    "    else:\n",
    "        items = []\n",
    "        iteration = \"Item name = \" + str(search_keyword)\n",
    "        print (iteration)\n",
    "        search = search_keyword.replace(' ','%20')\n",
    "\n",
    "        url = 'https://www.google.com/search?q=' + search +  '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "        raw_html =  (download_page(url))\n",
    "        items = items + (get_all_image_links(raw_html))\n",
    "\n",
    "        print (\"Image Links = \"+str(items))\n",
    "        print (\"Total Image Links = \"+str(len(items)))\n",
    "        print (\"\\n\")\n",
    "\n",
    "        info = open('output.txt', 'a')        #Open the text file called database.txt\n",
    "        info.write(str(search_keyword) + \": \" + str(items) + \"\\n\\n\\n\")         #Write the title of the page\n",
    "        info.close()                            #Close the file\n",
    "\n",
    "\n",
    "        if len(arg)>1 and arg[1] == \"download\":\n",
    "            i = 0\n",
    "            while i<len(items):\n",
    "                link = items[i]\n",
    "                try:\n",
    "                    file = urllib.URLopener()\n",
    "                    file.retrieve(link, str(\"img \"+str(i)+\".jpg\"))\n",
    "                    print(\"downloaded --> \"+ link)\n",
    "                except:\n",
    "                    pass\n",
    "                i = i+1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "###### Save Wikipedia Articles (extract only the text of the article) ######\n",
    "def save_wikipedia_article(url,*arg):\n",
    "    raw_page = download_page(url)\n",
    "    start_heading = raw_page.find('<h1 id=\"firstHeading\"')\n",
    "    end_start_heading = raw_page.find('>',start_heading+1)\n",
    "    end_heading = raw_page.find('</h1>',end_start_heading+1)\n",
    "    heading = raw_page[end_start_heading+1:end_heading]\n",
    "    page = heading + '\\n\\n'\n",
    "    raw_page = raw_page[end_heading+5:]\n",
    "    para_count = 0      #Initiate Para Count\n",
    "    while True:\n",
    "        find_paragraph = raw_page.find('<p>')\n",
    "        find_heading = raw_page.find('<span class=\"mw-headline\"')\n",
    "        if find_paragraph == -1 or find_heading == -1:\n",
    "            break\n",
    "        else:\n",
    "            if find_paragraph < find_heading:\n",
    "                #extract paragraph\n",
    "                start_paragraph = raw_page.find('<p>')\n",
    "                end_paragraph = raw_page.find('</p>',start_paragraph+1)\n",
    "                paragraph_raw = raw_page[start_paragraph+3:end_paragraph]\n",
    "                #remove HTML tags\n",
    "                paragraph_2 = (re.sub(r'<.+?>', '', paragraph_raw))\n",
    "                #remove citations\n",
    "                paragraph = (re.sub(r'\\[.*?\\]', '', paragraph_2))\n",
    "                #update paragraph count\n",
    "                para_count += 1\n",
    "                #add paragraph to the 'page'\n",
    "                if para_count < 2:\n",
    "                    page = page + '\\n' + paragraph\n",
    "                else:\n",
    "                    page = page + '\\n\\n' + paragraph\n",
    "                #reduce the raw_page size\n",
    "                raw_page = raw_page[end_paragraph:]\n",
    "            else:\n",
    "                #extract heading\n",
    "                start_heading = raw_page.find('<span class=\"mw-headline\"')\n",
    "                end_start_heading = raw_page.find('>',start_heading+1)\n",
    "                end_heading = raw_page.find('</span>',start_heading+1)\n",
    "                heading_raw = raw_page[end_start_heading+1:end_heading]\n",
    "                #remove HTML tags\n",
    "                heading = (re.sub(r'<.+?>', '', heading_raw))#.replace('\\n', '')\n",
    "                #Since heading appeared, reset the para count\n",
    "                para_count = 0\n",
    "                #add heading to the 'page'\n",
    "                page = page + '\\n\\n' + heading + ':'\n",
    "                #reduce the raw_page size\n",
    "                raw_page = raw_page[end_heading:]\n",
    "    if len(arg)>0:\n",
    "        file = open(arg[0],'w')\n",
    "        file.write(page)\n",
    "        file.close()\n",
    "        return page\n",
    "    else:\n",
    "        return page\n",
    "\n",
    "\n",
    "\n",
    "###### Wikipedia Crawler ######\n",
    "#URL parsing for incomplete or duplicate URLs\n",
    "def wikipedia_url_parse(url):\n",
    "    seed_page = \"https://en.wikipedia.org\"  #Crawling the English Wikipedia\n",
    "    try:\n",
    "        from urllib.parse import urlparse\n",
    "    except ImportError:\n",
    "        from urlparse import urlparse\n",
    "    url = url  #.lower()    #Make it lower case\n",
    "    s = urlparse(url)       #parse the given url\n",
    "    seed_page_n = seed_page #.lower()       #Make it lower case\n",
    "    #t = urlparse(seed_page_n)     #parse the seed page (reference page)\n",
    "    i = 0\n",
    "    flag = 0\n",
    "    while i<=9:\n",
    "        if url == \"/\":\n",
    "            url = seed_page_n\n",
    "            flag = 0\n",
    "        elif not s.scheme:\n",
    "            url = \"http://\" + url\n",
    "            flag = 0\n",
    "        elif \"#\" in url:\n",
    "            url = url[:url.find(\"#\")]\n",
    "            flag = 0\n",
    "        elif \"?\" in url:\n",
    "            url = url[:url.find(\"?\")]\n",
    "            flag = 0\n",
    "        elif s.netloc == \"\":\n",
    "            url = seed_page + s.path\n",
    "            flag = 0\n",
    "        elif url[len(url)-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "            flag = 0\n",
    "        else:\n",
    "            url = url\n",
    "            flag = 0\n",
    "            break\n",
    "        i = i+1\n",
    "        s = urlparse(url)   #Parse after every loop to update the values of url parameters\n",
    "    return(url, flag)\n",
    "\n",
    "#Main Crawl function that calls all the above function and crawls the entire site sequentially\n",
    "def wikipedia_crawl(starting_page,*arg):\n",
    "    to_crawl = [starting_page]      #Define list name 'Seed Page'\n",
    "    crawled=[]      #Define list name 'Seed Page'\n",
    "    i=0        #Initiate Variable to count No. of Iterations\n",
    "    while i<arg[0]:     #Continue Looping till the 'to_crawl' list is not empty\n",
    "        urll = to_crawl.pop(0)      #If there are elements in to_crawl then pop out the first element\n",
    "        urll,flag = wikipedia_url_parse(urll)\n",
    "        #print(urll)\n",
    "        flag2 = extension_scan(urll)\n",
    "        time.sleep(1)\n",
    "\n",
    "        #If flag = 1, then the URL is outside the seed domain URL\n",
    "        if flag == 1 or flag2 == 1:\n",
    "            pass        #Do Nothing\n",
    "\n",
    "        else:\n",
    "            if urll in crawled:     #Else check if the URL is already crawled\n",
    "                pass        #Do Nothing\n",
    "            else:       #If the URL is not already crawled, then crawl i and extract all the links from it\n",
    "                raw_html = download_page(urll)\n",
    "                #print(raw_html)\n",
    "\n",
    "                start_heading = raw_html.find('<h1 id=\"firstHeading\"')\n",
    "                end_start_heading = raw_html.find('>',start_heading+1)\n",
    "                end_heading = raw_html.find('</h1>',end_start_heading+1)\n",
    "                heading = raw_html[end_start_heading+1:end_heading]\n",
    "                heading = heading.replace('<i>', '').replace('</i>','')\n",
    "\n",
    "                print(\"Title = \" + heading)\n",
    "                print(\"Link = \" + urll)\n",
    "                to_crawl = to_crawl + find_all_links(raw_html)\n",
    "                if len(to_crawl)>1000:\n",
    "                    to_crawl = to_crawl[:999]\n",
    "                crawled.append(urll)\n",
    "\n",
    "                #Remove duplicated from to_crawl\n",
    "                n = 1\n",
    "                j = 0\n",
    "                #k = 0\n",
    "                while j < (len(to_crawl)-n):\n",
    "                    if to_crawl[j] in to_crawl[j+1:(len(to_crawl)-1)]:\n",
    "                        to_crawl.pop(j)\n",
    "                        n = n+1\n",
    "                    else:\n",
    "                        pass     #Do Nothing\n",
    "                    j = j+1\n",
    "            i=i+1\n",
    "            print(\"Iteration No. = \" + str(i) + ' | ' + \"To Crawl = \" + str(len(to_crawl)) + ' | ' + \"Crawled = \" + str(len(crawled)) + '\\n')\n",
    "            #Writing the output data into a text file\n",
    "            if len(arg)>1:\n",
    "                file = open(arg[1], 'a')        #Open the text file called database.txt\n",
    "                file.write(\"Title = \" + heading + \"\\n\")         #Write the title of the page\n",
    "                file.write(\"Link = \" + urll + \"\\n\")\n",
    "                file.write(\"Iteration No. = \" + str(i) + ' | ' + \"To Crawl = \" + str(len(to_crawl)) + ' | ' + \"Crawled = \" + str(len(crawled)) + '\\n\\n')\n",
    "                file.close()                            #Close the file\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "#Google Seatch using the Search API\n",
    "def google_search(query):\n",
    "    query = urllib.urlencode ( { 'q' : query } )\n",
    "    response = urllib.urlopen ( 'http://ajax.googleapis.com/ajax/services/search/web?v=1.0&' + query ).read()\n",
    "    json = m_json.loads ( response )\n",
    "    results = json [ 'responseData' ] [ 'results' ]\n",
    "    for result in results:\n",
    "        title = result['title'].replace('<b>','').replace('</b>','')\n",
    "        link = result['url']\n",
    "        print (title + '; ' + link)\n",
    "\n",
    "########## End ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"EN\" lang=\"EN\" dir=\"ltr\">\\n<head profile=\"http://gmpg.org/xfn/11\">\\n\\n<title>Z Series Innovations</title>\\n\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\" />\\n<meta http-equiv=\"imagetoolbar\" content=\"no\" />\\n<meta name=\"google-site-verification\" content=\"IeOstBf2uOEP3FodUIymkONv0XBqT7vkEqenOdUd6Dk\" />\\n\\n<link rel=\"stylesheet\" href=\"styles/layout.css\" type=\"text/css\" />\\n\\n<link rel=\"stylesheet\" href=\"/styles/image gallery.css\" type=\"text/css\" />\\n\\n<script type=\"text/javascript\" src=\"scripts/jquery-1.4.1.min.js\"></script>\\n<script type=\"text/javascript\" src=\"scripts/jquery.jcarousel.pack.js\"></script>\\n<script type=\"text/javascript\" src=\"scripts/jquery.jcarousel.setup.js\"></script>\\n\\n\\n\\n<!-- Description  ##################################################################################################################################### -->\\n<meta name=\"description\" content=\"Library of all the technical things\">\\n\\n<meta name=\"keywords\" content=\"Z Series Innovations, zseries, innovations, technical, engineering, electronics, telecommunication, computer science, robotics, embedded, projects, technical articles, tutorials, quizzes\">\\n\\n<meta name=\"author\" content=\"Z Series Innovations\">\\n\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\\n\\n\\n<!-- Title  ##################################################################################################################################### -->\\n<title>Z Series Innovations | Homepage </title>\\n\\n\\n<LINK REL=\"SHORTCUT ICON\"HREF=\"/images/zseries logo.ico\">\\n\\n\\n<!-- Google Analytics Code -->\\n<script type=\"text/javascript\">\\n\\n  var _gaq = _gaq || [];\\n  _gaq.push([\\'_setAccount\\', \\'UA-33112066-2\\']);\\n  _gaq.push([\\'_setDomainName\\', \\'zseries.in\\']);\\n  _gaq.push([\\'_trackPageview\\']);\\n\\n  (function() {\\n    var ga = document.createElement(\\'script\\'); ga.type = \\'text/javascript\\'; ga.async = true;\\n    ga.src = (\\'https:\\' == document.location.protocol ? \\'https://ssl\\' : \\'http://www\\') + \\'.google-analytics.com/ga.js\\';\\n    var s = document.getElementsByTagName(\\'script\\')[0]; s.parentNode.insertBefore(ga, s);\\n  })();\\n\\n</script>\\n\\n\\n\\n\\n\\n<!-- Add jQuery library -->\\n<script type=\"text/javascript\" src=\"http://code.jquery.com/jquery-latest.min.js\"></script>\\n\\n<!-- Add mousewheel plugin (this is optional) -->\\n<script type=\"text/javascript\" src=\"/fancybox/lib/jquery.mousewheel-3.0.6.pack.js\"></script>\\n\\n<!-- Add fancyBox -->\\n<link rel=\"stylesheet\" href=\"/fancybox/source/jquery.fancybox.css\" type=\"text/css\" media=\"screen\" />\\n<script type=\"text/javascript\" src=\"/fancybox/source/jquery.fancybox.pack.js\"></script>\\n\\n<!-- Optionally add helpers - button, thumbnail and/or media -->\\n<link rel=\"stylesheet\" href=\"/fancybox/source/helpers/jquery.fancybox-buttons.css?v=1.0.5\" type=\"text/css\" media=\"screen\" />\\n<script type=\"text/javascript\" src=\"/fancybox/source/helpers/jquery.fancybox-buttons.js\"></script>\\n<script type=\"text/javascript\" src=\"/fancybox/source/helpers/jquery.fancybox-media.js\"></script>\\n\\n<link rel=\"stylesheet\" href=\"/fancybox/source/helpers/jquery.fancybox-thumbs.css\" type=\"text/css\" media=\"screen\" />\\n<script type=\"text/javascript\" src=\"/fancybox/source/helpers/jquery.fancybox-thumbs.js\"></script>\\n\\n\\n<script>\\n\\n$(document).ready(function() {\\n\\t$(\".various\").fancybox({\\n\\t\\tmaxWidth\\t: 800,\\n\\t\\tmaxHeight\\t: 600,\\n\\t\\tfitToView\\t: false,\\n\\t\\twidth\\t\\t: \\'560\\',\\n\\t\\theight\\t\\t: \\'315\\',\\n\\t\\tautoSize\\t: true,\\n\\t\\tcloseClick\\t: false,\\n\\t\\topenEffect\\t: \\'elastic\\',\\n\\t\\tcloseEffect\\t: \\'elastic\\'\\n\\t});\\n});\\n</script>\\n\\n\\n</head>\\n\\n<body id=\"top\">\\n\\n\\n\\n<div class=\"wrapper col1\">\\n  <div id=\"header\">\\n    <div id=\"logo\">\\n    \\t<img border=\"0\" src=\"/images/zseries logo.png\" alt=\"Z Series Logo\" width=\"180\" height=\"65\" />\\n    </div>\\n    <div id=\"topnav\">\\n      <ul>\\n        <li class=\"active\"><a href=\"/\">Home</a></li>\\n\\t<li><a href=\"#\">Laboratories</a>\\n\\t<ul>\\n            <li><a href=\"/robotics lab\">Robotics Lab</a></li>\\n            <li><a href=\"/electronics lab\">Electronics Lab</a></li>\\n            <li><a href=\"/embedded lab\">Embedded Lab</a></li>\\n            <li><a href=\"/vlsi lab\">VLSI Lab</a></li>\\n            <li><a href=\"/cyber lab\">Cyber Lab</a></li>\\n            <li><a href=\"/astronomy lab\">Astronomy Lab</a></li>\\n            <li><a href=\"/mathematics lab\">Mathematics Lab</a></li>\\n            <li><a href=\"/telecom lab\">Telecomunication Lab</a></li>\\n            <li><a href=\"/computer science lab\">Computer Science Lab</a></li>\\n            \\n          \\t    \\n\\t</ul>\\n\\t</li>\\n        <li><a href=\"/quizzes/index.php\">Quizzes</a></li>\\n        <li><a href=\"/extras/sitemap.php\">Sitemap</a></li>\\n\\t<li class=\"last\"><a href=\"/extras/contact us.php\">Contact Us</a></li>\\n      </ul>\\n    </div>\\n    <br class=\"clear\" />\\n  </div>\\n</div>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Location  ################################################################################################################################## -->\\n<div class=\"wrapper col2\">\\n  <div id=\"featured_slide\">\\n    <div id=\"featured_content\">\\n      <ul>\\n        <li><a class=\"various fancybox.iframe\" href=\"http://www.youtube.com/embed/Gs-3ZNGkX_c?autoplay=1\"><img src=\"images/homepage/promo.png\" alt=\"\" /></a>\\n          <div class=\"floater\">\\n            <h2>About This Website!</h2>\\n            <p>This Engineering portal of <a href=\"http://www.zseries.in/\">Z Series Innovations</a> hosts hundreds of quality <a href=\"http://www.zseries.in/extras/sitemap.php\">articles</a> on each major field of Engineering. Learn through our tons of <a href=\"https://www.youtube.com/user/beyondourknowledge\">video tutorials</a> and animations. Also play exciting <a href=\"http://www.zseries.in/quizzes/index.php\">quizzes</a> to test your knowledge. Enjoy learning! </p> <b>\"Engineering is the core science that needs to be perceived\"</b>\\n            \\n          </div>\\n        </li>\\n        \\n      </ul>\\n    </div>\\n     </div>\\n</div>\\n\\n\\n\\n\\n\\n<div class=\"wrapper col3\">\\n  <div id=\"container\">\\n\\n<h1 style align=\"center\" font-size=\"45px\"><big>Laboratories - Learn, Innovate and Apply!</big></h1>\\n\\n<div class=\"polaroid\">\\n  <p><br>Electronics Lab</p>\\n  <a href=\"http://www.zseries.in/electronics lab\"><img src=\"http://zseries.in/images/thumbnails/electronics.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n   \\n\\n    \\n<div class=\"polaroid\">\\n  <p><br>Robotics Lab</p>\\n  <a href=\"http://www.zseries.in/robotics lab\"><img src=\"http://zseries.in/images/thumbnails/robotics.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>Embedded Lab</p>\\n  <a href=\"http://www.zseries.in/embedded lab\"><img src=\"http://zseries.in/images/thumbnails/embedded.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>Computer Science Lab</p>\\n  <a href=\"http://www.zseries.in/computer science lab\"><img src=\"http://zseries.in/images/thumbnails/computer.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>Telecom Lab</p>\\n  <a href=\"http://www.zseries.in/telecom lab\"><img src=\"http://zseries.in/images/thumbnails/telecom.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>Mathematics Lab</p>\\n  <a href=\"http://www.zseries.in/mathematics lab\"><img src=\"http://zseries.in/images/thumbnails/mathematics.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>VLSI Lab</p>\\n  <a href=\"http://www.zseries.in/vlsi lab\"><img src=\"http://zseries.in/images/thumbnails/vlsi.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>Astronomy Lab</p>\\n  <a href=\"http://www.zseries.in/astronomy lab\"><img src=\"http://zseries.in/images/thumbnails/astronomy.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n\\n<div class=\"polaroid\">\\n  <p><br>Cyber Lab</p>\\n  <a href=\"http://www.zseries.in/cyber lab\"><img src=\"http://zseries.in/images/thumbnails/cyber.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n\\n\\n\\n\\n\\n<!--\\n<div class=\"polaroid\">\\n  <p><br>Quizzes</p>\\n  <a href=\"http://www.zseries.in/quizzes\"><img src=\"http://zseries.in/images/thumbnails/quiz.jpg\" height=\"150px\" width=\"270px\"></a>\\n</div>\\n-->\\n      <br class=\"clear\">\\n    </div>\\n  </div>\\n</div>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<br><br><br>\\n\\n</div>\\n\\n<div class=\"wrapper col3\">\\n  <div id=\"container\">  \\n\\n<h1><big><big>Testimonial</big></big></h1>\\n<iframe src=\"/extras/testimonials/testimonials.php\" style=\"width:100%; height:130px;  border: none;\"></iframe>\\n</div>\\n</div>\\n\\n\\n<div class=\"wrapper col4\">\\n  <div id=\"footer\">\\n    \\n    \\n\\n<div class=\"box1\">\\n      <h2>About Us!</h2>\\n     <img class=\"imgl\" src=\"/images/bottom/short logo.png\" alt=\"\" width=\"70\" height=\"60\" />\\n      <p>\\'Z Series Innovations\\' is an e-learning solution for learning all technical stuff online. It is an online portal that gives an enhanced way of learning and guidance in various fields of engineering which include robotics, electronics, communication,computer science, embedded systems and its real life applications.</p>\\n \\n<a href=\"/extras/about us.php\">\\n<img class=\"imgl\" src=\"/images/homepage/read more.png\" alt=\"\" width=\"110\" height=\"30\" /></a>\\n</div>\\n    \\n    \\n\\n\\n\\n<div class=\"box contactdetails\">\\n      <h2>Join Us</h2>\\n      Do you feel like contributing to this website technically? Jump in!\\n<br><br> Your technical expertise would be needed to create quality articles and content on this site and hence contribute to this growing global community of technical enthusiast.\\n<br><br>\\nCreate quality content and get recognized!<br>\\n\\n<a href=\"/extras/contact us.php\">\\n<img class=\"imgl\" src=\"/images/homepage/join us.png\" alt=\"\" width=\"110\" height=\"30\" /></a>\\n\\n</div>\\n    \\n\\n    \\n\\n\\n\\n<div class=\"box flickrbox\">\\n      <h2>Our Works!</h2>\\n      <div class=\"wrap\">\\n        <iframe width=\"330\" height=\"186\" src=\"//www.youtube.com/embed/aKs5vs07yJg\" frameborder=\"0\" allowfullscreen></iframe>\\n      </div>\\n    </div>\\n    <br class=\"clear\" />\\n  </div>\\n</div>\\n\\n\\n\\n<div class=\"footer\">\\n<h3>Z Series Innovations &copy; 2011-2015</h3>\\n</div>\\n\\n</div><!--wrapper ends-->\\n\\n<br /></body></html>\\n\\n\\n</body>\\n</html>'\n",
      "\n",
      "Clean Page: b'\\n\\n\\n\\nZ Series Innovations\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nZ Series Innovations | Homepage \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    \\t\\n    \\n    \\n      \\n        Home\\n\\tLaboratories\\n\\t\\n            Robotics Lab\\n            Electronics Lab\\n            Embedded Lab\\n            VLSI Lab\\n            Cyber Lab\\n            Astronomy Lab\\n            Mathematics Lab\\n            Telecomunication Lab\\n            Computer Science Lab\\n            \\n          \\t    \\n\\t\\n\\t\\n        Quizzes\\n        Sitemap\\n\\tContact Us\\n      \\n    \\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n      \\n        \\n          \\n            About This Website!\\n            This Engineering portal of Z Series Innovations hosts hundreds of quality articles on each major field of Engineering. Learn through our tons of video tutorials and animations. Also play exciting quizzes to test your knowledge. Enjoy learning!  \"Engineering is the core science that needs to be perceived\"\\n            \\n          \\n        \\n        \\n      \\n    \\n     \\n\\n\\n\\n\\n\\n\\n\\n  \\n\\nLaboratories - Learn, Innovate and Apply!\\n\\n\\n  Electronics Lab\\n  \\n\\n   \\n\\n    \\n\\n  Robotics Lab\\n  \\n\\n\\n\\n\\n\\n  Embedded Lab\\n  \\n\\n\\n\\n\\n\\n  Computer Science Lab\\n  \\n\\n\\n\\n\\n  Telecom Lab\\n  \\n\\n\\n\\n\\n  Mathematics Lab\\n  \\n\\n\\n\\n\\n  VLSI Lab\\n  \\n\\n\\n\\n\\n\\n  Astronomy Lab\\n  \\n\\n\\n\\n\\n\\n  Cyber Lab\\n  \\n\\n\\n\\n\\n\\n\\n\\n  Quizzes\\n  \\n\\n-->\\n      \\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\nTestimonial\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    \\n\\n\\n      About Us!\\n     \\n      \\'Z Series Innovations\\' is an e-learning solution for learning all technical stuff online. It is an online portal that gives an enhanced way of learning and guidance in various fields of engineering which include robotics, electronics, communication,computer science, embedded systems and its real life applications.\\n \\n\\n\\n\\n    \\n    \\n\\n\\n\\n\\n      Join Us\\n      Do you feel like contributing to this website technically? Jump in!\\n Your technical expertise would be needed to create quality articles and content on this site and hence contribute to this growing global community of technical enthusiast.\\n\\nCreate quality content and get recognized!\\n\\n\\n\\n\\n\\n    \\n\\n    \\n\\n\\n\\n\\n      Our Works!\\n      \\n        \\n      \\n    \\n    \\n  \\n\\n\\n\\n\\n\\nZ Series Innovations &copy; 2011-2015\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "import webb\n",
    "\n",
    "page = webb.download_page(\"http://www.zseries.in\")\n",
    "print(page)\n",
    "clea_page = webb.clean_page(page)\n",
    "print()\n",
    "print('Clean Page: '+ clea_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item name = https://imgd.aeplcdn.com/642x361/n/cw/ec/46650/marutisuzuki-swift-exterior0.jpeg?q=85\n",
      "Image Links = []\n",
      "Total Image Links = 0\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Importing the 'webb' library\n",
    "import webb\n",
    "\n",
    "#Downloading the web page and printing it\n",
    "webb.download_google_images(\"Flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
